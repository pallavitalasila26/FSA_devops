{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be303ee",
   "metadata": {},
   "source": [
    "## Notebook set up\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8422b",
   "metadata": {},
   "source": [
    "## Exercise 1: Generate and explore the dataset\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "1. Use `make_friedman1()` to generate a regression dataset with 1000 samples and 5 features. Set `random_state=315` for reproducibility. Store the features in `X` and the target in `y`.\n",
    "\n",
    "2. Convert `X` and `y` to a pandas DataFrame and Series respectively. Name the features `feature_0`, `feature_1`, etc., and name the target `target`.\n",
    "\n",
    "3. Display basic information about the dataset:\n",
    "   - Shape of features and target\n",
    "   - First few rows\n",
    "   - Summary statistics\n",
    "\n",
    "4. Create a figure with 5 subplots (one for each feature) showing scatter plots of each feature vs. the target.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- `make_friedman1()` returns features and target as numpy arrays\n",
    "  - Example: `X, y = make_friedman1(n_samples=100, n_features=5, random_state=315)`\n",
    "\n",
    "- To convert to pandas:\n",
    "  - `X_df = pd.DataFrame(X, columns=['feature_0', 'feature_1', ...])`\n",
    "  - `y_series = pd.Series(y, name='target')`\n",
    "\n",
    "- The Friedman #1 regression problem generates output using the formula:\n",
    "  - y = 10 * sin(π * X₀ * X₁) + 20 * (X₂ - 0.5)² + 10 * X₃ + 5 * X₄ + noise\n",
    "  - This means the relationship is **non-linear** and only uses the first 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cf004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87753c18",
   "metadata": {},
   "source": [
    "## Exercise 2: Train and evaluate models\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "1. Split the data into training and testing sets using an 80-20 split. Use `random_state=315`.\n",
    "\n",
    "2. Train a `LinearRegression` model on the training data.\n",
    "\n",
    "3. Train a `DecisionTreeRegressor` model on the training data. Use `random_state=315`.\n",
    "\n",
    "4. For both models, calculate:\n",
    "   - Training RMSE\n",
    "   - Testing RMSE\n",
    "   - Training R² score\n",
    "   - Testing R² score\n",
    "\n",
    "5. Print a comparison table showing these metrics for both models.\n",
    "\n",
    "6. Create a figure with 2 subplots:\n",
    "   - Left: Scatter plot of true vs. predicted values for linear regression\n",
    "   - Right: Scatter plot of true vs. predicted values for decision tree\n",
    "   - Add a diagonal reference line (y=x) to each plot\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- Use `train_test_split()` with `test_size=0.2`\n",
    "  - Example: `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=315)`\n",
    "\n",
    "- To calculate metrics:\n",
    "  - First make predictions: `y_pred = model.predict(X_test)`\n",
    "  - Then calculate: `rmse = root_mean_squared_error(y_test, y_pred)`\n",
    "\n",
    "- To add a reference line to a plot:\n",
    "  - `plt.plot([min, max], [min, max], 'k--', alpha=0.3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e9cd5",
   "metadata": {},
   "source": [
    "## Exercise 3: Investigate model performance with cross-validation\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "1. Use 5-fold cross-validation to evaluate both models on the full dataset. Use `neg_root_mean_squared_error` as the scoring metric.\n",
    "\n",
    "2. Create a boxplot comparing the cross-validation RMSE scores for both models.\n",
    "\n",
    "3. Calculate and print the mean and standard deviation of the cross-validation scores for each model.\n",
    "\n",
    "4. Answer the following questions:\n",
    "   - Which model performs better overall?\n",
    "   - Is there a significant difference in performance?\n",
    "   - Does either model show signs of overfitting? (Hint: compare training vs. testing performance from Exercise 2)\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- Use `cross_val_score()` with `cv=5` and `scoring='neg_root_mean_squared_error'`\n",
    "  - Example: `scores = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')`\n",
    "  - Note: sklearn returns negative scores, so multiply by -1 to get positive RMSE values\n",
    "\n",
    "- To create a boxplot of multiple datasets:\n",
    "  - `plt.boxplot([scores1, scores2], labels=['Model 1', 'Model 2'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b98c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0a701",
   "metadata": {},
   "source": [
    "## Exercise 4: Investigate why the models perform differently\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "1. Create visualizations to understand the relationship between features and target:\n",
    "   - For features 0 and 1: Create a 2D scatter plot colored by the target value (use a colormap)\n",
    "   - For features 2, 3, and 4: Create individual scatter plots vs. target\n",
    "\n",
    "2. Based on the Friedman #1 formula (y = 10 * sin(π * X₀ * X₁) + 20 * (X₂ - 0.5)² + 10 * X₃ + 5 * X₄ + noise):\n",
    "   - Identify which relationships are linear\n",
    "   - Identify which relationships are non-linear\n",
    "   - Explain how this affects each model's performance\n",
    "\n",
    "3. Create residual plots for both models (predicted vs. residuals):\n",
    "   - What patterns do you see in the linear regression residuals?\n",
    "   - What patterns do you see in the decision tree residuals?\n",
    "   - What do these patterns tell you about each model's ability to capture the underlying relationships?\n",
    "\n",
    "4. (Optional) Try to improve the linear regression model by adding polynomial features for the non-linear relationships. Does this improve performance?\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- For a 2D scatter plot with color mapping:\n",
    "  - `plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')`\n",
    "  - `plt.colorbar(label='Target')`\n",
    "\n",
    "- Residuals are calculated as: `residuals = y_true - y_predicted`\n",
    "\n",
    "- The decision tree can capture non-linear relationships by splitting the feature space, while linear regression assumes linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74694d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef9a87",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Based on your analysis, answer the following questions:\n",
    "\n",
    "1. **Model performance**: Which model performed better and why?\n",
    "\n",
    "2. **Linear assumptions**: What happens when you apply linear regression to non-linear data?\n",
    "\n",
    "3. **Model complexity**: What are the trade-offs between simpler models (linear regression) and more complex models (decision trees)?\n",
    "\n",
    "4. **Real-world implications**: In what situations would you prefer:\n",
    "   - A linear regression model?\n",
    "   - A decision tree model?\n",
    "   - Consider factors like interpretability, performance, and data characteristics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
